{
    "docs": [
        {
            "location": "/",
            "text": "Introduction\n\n\nmini-kep\n is a small ETL (extract, transform, load) framework for \nRussian and global macroeconomic time series data with public end-user API.\n\n\nmini-kep\n collects data from static files and public APIs,\nsaves it in database and provides end-user interface to browse this data \nand read it into R/pandas for visualisation and modelling.  \n\n\nInspired by \nSt Louis FRED\n and \n\nData Science Cookiecutter\n,\nand aims to provide open, timely, machine-readable data for reproducible \nanalysis in economics.\n\n\nEnd use example\n\n\nRead official daily ruble/usd exchange rate from start of 2017\n\n\nimport pandas as pd\n\ndef read_ts(source_url):\n    \"\"\"Read pandas time series from *source_url*.\"\"\"\n    return pd.read_csv(source_url, \n                      converters={0: pd.to_datetime}, \n                      index_col=0,\n                      squeeze=True)\n\ner = read_ts('http://mini-kep.herokuapp.com/ru/series/USDRUR_CB/d/2017')\nassert er['2017-09-28'] == 58.01022\n\n\n\n\n\nClick to see data in browser:\n\nhttp://mini-kep.herokuapp.com/ru/series/USDRUR_CB/d/2017\n\n\nProject repositories and links\n\n\nParsers\n:\ndata source descriptions and parser code.\n\n\nDatabase layer\n:\nAPI and database for data storage and \nretrieval\n, \nhosted at\n\n\nhttps://minikep-db.herokuapp.com/api/\n\n\nFrontend app\n:\ncan be viewed in browser at \nhttps://mini-kep.herokuapp.com\n, relies on API above. \n\n\nEnd user code\n:\n\u0430 collection of Jupiter notebooks to demostrate data use.",
            "title": "Overview"
        },
        {
            "location": "/#introduction",
            "text": "mini-kep  is a small ETL (extract, transform, load) framework for \nRussian and global macroeconomic time series data with public end-user API.  mini-kep  collects data from static files and public APIs,\nsaves it in database and provides end-user interface to browse this data \nand read it into R/pandas for visualisation and modelling.    Inspired by  St Louis FRED  and  Data Science Cookiecutter ,\nand aims to provide open, timely, machine-readable data for reproducible \nanalysis in economics.",
            "title": "Introduction"
        },
        {
            "location": "/#end-use-example",
            "text": "Read official daily ruble/usd exchange rate from start of 2017  import pandas as pd\n\ndef read_ts(source_url):\n    \"\"\"Read pandas time series from *source_url*.\"\"\"\n    return pd.read_csv(source_url, \n                      converters={0: pd.to_datetime}, \n                      index_col=0,\n                      squeeze=True)\n\ner = read_ts('http://mini-kep.herokuapp.com/ru/series/USDRUR_CB/d/2017')\nassert er['2017-09-28'] == 58.01022  Click to see data in browser: http://mini-kep.herokuapp.com/ru/series/USDRUR_CB/d/2017",
            "title": "End use example"
        },
        {
            "location": "/#project-repositories-and-links",
            "text": "Parsers :\ndata source descriptions and parser code.  Database layer :\nAPI and database for data storage and  retrieval , \nhosted at  https://minikep-db.herokuapp.com/api/  Frontend app :\ncan be viewed in browser at  https://mini-kep.herokuapp.com , relies on API above.   End user code :\n\u0430 collection of Jupiter notebooks to demostrate data use.",
            "title": "Project repositories and links"
        },
        {
            "location": "/usercase/",
            "text": "User case notes\n\n\nFrom readme\n\n\nWe assume an end user has some experience with data from FRED or quandl and for his work he wants:\n\n\n\n\na clean dataset with latest data from different sources\n\n\nbrowse what data is available\n\n\nread this data on a local machine:\n\n\nas pd.DataFrame \n\n\nas R dataframe  \n\n\nquickly draw some charts like one below: \n\n\n\n\n\n\nEnduser code for data access\n\n\nEnd-user API calls should be like:\n\n\n\n# monthly average Brent oil prices starting 2000 to present \nbrent = pd.read_json('http://minikep.cc/oil/series/BRENT/m/avg/2000')\n\n# monthly average Russian rouble exchange rate, same period\ner = pd.read_json('http://minikep.cc/ru/series/USDRUR/m/avg/2000')\n\n# monthly consumer inflation in different countries in 2017\n# rog = rate of growth \ncpi_ru = pd.read_json('http://minikep.cc/ru/series/CPI/m/rog/2017')\ncpi_us = pd.read_json('http://minikep.cc/us/series/CPI/m/rog/2017')\n\n\n\n\n\nAdvanced usage\n\n\nServices based on mini-kep API: \n- PDF handout charts\n\n- datasets/presets\n- forcasts as a service (via API)\n- FB posts with data\n\n\nNotebooks catalog\n\n\nWe need to demonstarte usage of data from custom or standard API for visualisations and modelling.\n\n\nThere is a \nrepo for use cases\n, but it is practically empty yet.\n\n\nThe use cases may resemble \ndatachart.cc\n\nor \ndatalab\n\nor \nviz-demo\n or some other repos (eg cmf-comovements).",
            "title": "User case"
        },
        {
            "location": "/usercase/#user-case-notes",
            "text": "",
            "title": "User case notes"
        },
        {
            "location": "/usercase/#from-readme",
            "text": "We assume an end user has some experience with data from FRED or quandl and for his work he wants:   a clean dataset with latest data from different sources  browse what data is available  read this data on a local machine:  as pd.DataFrame   as R dataframe    quickly draw some charts like one below:",
            "title": "From readme"
        },
        {
            "location": "/usercase/#enduser-code-for-data-access",
            "text": "End-user API calls should be like:  \n# monthly average Brent oil prices starting 2000 to present \nbrent = pd.read_json('http://minikep.cc/oil/series/BRENT/m/avg/2000')\n\n# monthly average Russian rouble exchange rate, same period\ner = pd.read_json('http://minikep.cc/ru/series/USDRUR/m/avg/2000')\n\n# monthly consumer inflation in different countries in 2017\n# rog = rate of growth \ncpi_ru = pd.read_json('http://minikep.cc/ru/series/CPI/m/rog/2017')\ncpi_us = pd.read_json('http://minikep.cc/us/series/CPI/m/rog/2017')",
            "title": "Enduser code for data access"
        },
        {
            "location": "/usercase/#advanced-usage",
            "text": "Services based on mini-kep API: \n- PDF handout charts \n- datasets/presets\n- forcasts as a service (via API)\n- FB posts with data",
            "title": "Advanced usage"
        },
        {
            "location": "/usercase/#notebooks-catalog",
            "text": "We need to demonstarte usage of data from custom or standard API for visualisations and modelling.  There is a  repo for use cases , but it is practically empty yet.  The use cases may resemble  datachart.cc \nor  datalab \nor  viz-demo  or some other repos (eg cmf-comovements).",
            "title": "Notebooks catalog"
        },
        {
            "location": "/parsers/",
            "text": "Data sources and parsers\n\n\nData sources are static files (Word, PDF, html) and some open APIs. Parser is the python code performing requests and emitting datapoints. \n\n\nparser-rosstat-kep\n supplies most time series. It is supplemented by daily ruble exchange rate from Bank of Russia and oil prices from EIA and some others.\n\n\nAggratation of parser data is made at \nparsers/runner.py\n\n\nparser-rosstat-kep\n:\n\n \n\n\n\nparsers\n:",
            "title": "Parsers"
        },
        {
            "location": "/parsers/#data-sources-and-parsers",
            "text": "Data sources are static files (Word, PDF, html) and some open APIs. Parser is the python code performing requests and emitting datapoints.   parser-rosstat-kep  supplies most time series. It is supplemented by daily ruble exchange rate from Bank of Russia and oil prices from EIA and some others.  Aggratation of parser data is made at  parsers/runner.py  parser-rosstat-kep :    parsers :",
            "title": "Data sources and parsers"
        },
        {
            "location": "/scheduler/",
            "text": "Scheduler\n\n\nScheduler is not implemented yet, but it is a cron-like task list to invoke parsers and upload data to database. \n\n\nConsider:\n - \nHeroku APS\n\n - celery?\n - APScheduler?\n - something else?",
            "title": "Scheduler"
        },
        {
            "location": "/scheduler/#scheduler",
            "text": "Scheduler is not implemented yet, but it is a cron-like task list to invoke parsers and upload data to database.   Consider:\n -  Heroku APS \n - celery?\n - APScheduler?\n - something else?",
            "title": "Scheduler"
        },
        {
            "location": "/database/",
            "text": "Overview\n\n\nThis document describes database layer in between parsers and end-user API.\n\n\nExpected fucntionality:\n- parser delivers a list of dicts, each dict is a datapoint\n- database should have a POST method at \napi\\incoming\n and write incoming json to db\n- POST operation must have some authentication\n- for simplicity all data is upserted - newer data always overwrites older data\n- database has GET method has datapoint keys and parameters (variable name, frequency), \n- GET retruns same format of output as incoming, output ordered by date,  optionally  \n\n  filtered by start date and end date \n\n\nDatabase schema\n\n\nTable \nDatpoint\n\n\nId \u2013 UID, autoincrement  \nName \u2013 type String \\*  eg GDP\nFreq \u2013 type String \\*  \nDate \u2013 type DateTime \\*  2016-12-31\nValue \u2013 Float  \n\n\\* - composite key\n\n\n\n\nSee example at \nhttps://github.com/mini-kep/db/blob/master/demo/sqlalchemy/datapoint.py\n\n\nData structures\n\n\nData structure - incoming json\n\n\nIncoming json should have a structure like\n    [{\n        \"date\": \"1999-03-31\",\n        \"freq\": \"q\",\n        \"name\": \"INVESTMENT_bln_rub\",\n        \"value\": 12345.6\n    },\n    {...} \n    ]\n\n\nDatabase methods\n\n\nPOST\n\n\nPOST api/incoming\n \n\n\nValidates incoming json and upsert values to database. All fields should be filled.\n\n\nFor insert_many() operation see \nsheep/flock\n example at \nhttps://stackoverflow.com/a/33768160/1758363\n\n\nReturns:\n- empty JSON on success\n- error 400 if there\u2019s an error in incoming json (eg invalid date string or empty parameter or missing field)\n\n\nParser result is obtained by  \nDataset.yield_dicts(start='2017-01-01')\n in \nhttps://github.com/mini-kep/parsers/blob/master/parsers/runner.py\n, see \nDataset.serialise()\n for json creation.\n\n\nOther examples of incoming json:\n- a large (1.8M) json is \nlocated here\n\n- sample data is presented \nhere\n\n\nGET (REST)\n\n\nGET api/datapoints?name=<name>&freq=<freq>\nGET api/datapoints?name=<name>&freq=<freq>&start_date=<start_date>&end_date=<end_date>\n\n\n\n\nParameters:\n- name (required) \u2013 name value to search like name=BRENT\n- freq (required) \u2013 freq value to search like freq=m\n- start_date (optional) \u2013 should return results with date greater than this parameter\n- end_date (optional) \u2013 should return results with date less than this parameter\n- format (optional, possible values \njson, csv\n, default \ncsv\n) \u2013 returns data in chosen format. CSV data can be read by pandas with \npd.read_csv(url_to_api_request)\n\n\nReturns:\n- CSV or JSON (default CSV) in format similar to incoming json with data sorted by date\n- empty CSV or JSON if there\u2019s no data with such query.\n\n\nMethod validates parameters and returns error 400 if there\u2019s an error in parameters (like string in data parameter or empty parameter) \n\n\nSecurity\n\n\nPOST methods should require API_TOKEN as URL parameter or header, validate it with environment variable (possibly Heroku config vars)\n\n\nTests\n\n\nUpload data from JSON to DB, run python unit tests with requests to different methods, validate them with uploaded data.\nUse combinations GET \u2013 POST \u2013 GET to validate data inserts and updates.\n\nExample1\n\n\nExample2\n\n\nShould we write some tests in curl/httpie? http tests: \nhttps://github.com/mini-kep/db/blob/master/requests_tests.py\n\n\nTech stack\n\n\nWeb-frameworks: Flask + SQLAlchemy or Django, may consider Falcon\n\n\nContainer: prototype deployed to Heroku, may also use AWS\n\n\nDatabase: Postgres (default on Heroku) or RDS on AWS\n\n\nRepositories\n\n\nThe database layer is to be developed here, at \ndb\n repo: \nhttps://github.com/mini-kep/db\n\n\nflask \ndb\n: \n\n\n\nThere is also \ndjango project for database\n,  but it is on hold now.\n\n\nDiscussion\n\n\nhttps://github.com/mini-kep/db/issues/5",
            "title": "Database"
        },
        {
            "location": "/database/#overview",
            "text": "This document describes database layer in between parsers and end-user API.  Expected fucntionality:\n- parser delivers a list of dicts, each dict is a datapoint\n- database should have a POST method at  api\\incoming  and write incoming json to db\n- POST operation must have some authentication\n- for simplicity all data is upserted - newer data always overwrites older data\n- database has GET method has datapoint keys and parameters (variable name, frequency), \n- GET retruns same format of output as incoming, output ordered by date,  optionally   \n  filtered by start date and end date",
            "title": "Overview"
        },
        {
            "location": "/database/#database-schema",
            "text": "Table  Datpoint  Id \u2013 UID, autoincrement  \nName \u2013 type String \\*  eg GDP\nFreq \u2013 type String \\*  \nDate \u2013 type DateTime \\*  2016-12-31\nValue \u2013 Float  \n\n\\* - composite key  See example at  https://github.com/mini-kep/db/blob/master/demo/sqlalchemy/datapoint.py",
            "title": "Database schema"
        },
        {
            "location": "/database/#data-structures",
            "text": "",
            "title": "Data structures"
        },
        {
            "location": "/database/#data-structure-incoming-json",
            "text": "Incoming json should have a structure like\n    [{\n        \"date\": \"1999-03-31\",\n        \"freq\": \"q\",\n        \"name\": \"INVESTMENT_bln_rub\",\n        \"value\": 12345.6\n    },\n    {...} \n    ]",
            "title": "Data structure - incoming json"
        },
        {
            "location": "/database/#database-methods",
            "text": "",
            "title": "Database methods"
        },
        {
            "location": "/database/#post",
            "text": "POST api/incoming    Validates incoming json and upsert values to database. All fields should be filled.  For insert_many() operation see  sheep/flock  example at  https://stackoverflow.com/a/33768160/1758363  Returns:\n- empty JSON on success\n- error 400 if there\u2019s an error in incoming json (eg invalid date string or empty parameter or missing field)  Parser result is obtained by   Dataset.yield_dicts(start='2017-01-01')  in  https://github.com/mini-kep/parsers/blob/master/parsers/runner.py , see  Dataset.serialise()  for json creation.  Other examples of incoming json:\n- a large (1.8M) json is  located here \n- sample data is presented  here",
            "title": "POST"
        },
        {
            "location": "/database/#get-rest",
            "text": "GET api/datapoints?name=<name>&freq=<freq>\nGET api/datapoints?name=<name>&freq=<freq>&start_date=<start_date>&end_date=<end_date>  Parameters:\n- name (required) \u2013 name value to search like name=BRENT\n- freq (required) \u2013 freq value to search like freq=m\n- start_date (optional) \u2013 should return results with date greater than this parameter\n- end_date (optional) \u2013 should return results with date less than this parameter\n- format (optional, possible values  json, csv , default  csv ) \u2013 returns data in chosen format. CSV data can be read by pandas with  pd.read_csv(url_to_api_request)  Returns:\n- CSV or JSON (default CSV) in format similar to incoming json with data sorted by date\n- empty CSV or JSON if there\u2019s no data with such query.  Method validates parameters and returns error 400 if there\u2019s an error in parameters (like string in data parameter or empty parameter)",
            "title": "GET (REST)"
        },
        {
            "location": "/database/#security",
            "text": "POST methods should require API_TOKEN as URL parameter or header, validate it with environment variable (possibly Heroku config vars)",
            "title": "Security"
        },
        {
            "location": "/database/#tests",
            "text": "Upload data from JSON to DB, run python unit tests with requests to different methods, validate them with uploaded data.\nUse combinations GET \u2013 POST \u2013 GET to validate data inserts and updates. Example1  Example2  Should we write some tests in curl/httpie? http tests:  https://github.com/mini-kep/db/blob/master/requests_tests.py",
            "title": "Tests"
        },
        {
            "location": "/database/#tech-stack",
            "text": "Web-frameworks: Flask + SQLAlchemy or Django, may consider Falcon  Container: prototype deployed to Heroku, may also use AWS  Database: Postgres (default on Heroku) or RDS on AWS",
            "title": "Tech stack"
        },
        {
            "location": "/database/#repositories",
            "text": "The database layer is to be developed here, at  db  repo:  https://github.com/mini-kep/db  flask  db :   There is also  django project for database ,  but it is on hold now.",
            "title": "Repositories"
        },
        {
            "location": "/database/#discussion",
            "text": "https://github.com/mini-kep/db/issues/5",
            "title": "Discussion"
        },
        {
            "location": "/custom_api/",
            "text": "Overview\n\n\nCustom API is a simplified interface for end-user queries from database. \nIt uses long URL with slashes and no other parameters.\n\n\n\u0421ustom API is intended to allow:\n\n\n\n\nintuitive construction of URL for user\n\n\nshorter notation than standard database API GET method \n\n\naddressing several database API endpoints in one place\n\n\nuniform call to same indicator for different countries or regions\n\n\n\n\nAPI design originally discussed at \nthis issue\n.\n\n\nCustom API translates to db API\n\n\nCustom API is essentially a thin syntax layer on top of database API. \nAll calls to custom API are redirected to database API. \n\n\nFor example, this call to custom API: \n\n\nhttp://mini-kep.herokuapp.com/ru/series/CPI/m/rog/2015/2017\n\n\nwill return same data as:\n\n\nhttps://minikep-db.herokuapp.com/api/datapoints?name=CPI_rog&freq=m&start_date=2015-01-01&end_date=2017-12-31\n\n\nURL syntax\n\n\nCustom API url syntax is the following:\n\n\n{domain}/series/{varname}/{freq}/{?suffix}/{?start}/{?end}/{?finaliser}\n\n? - optional\n\nExamples:\n   oil/series/BRENT/m/eop/2015/2017/csv\n   ru/series/EXPORT_GOODS/m/bln_rub   \n\n\n\n\nFor further details, refer to the docstring in \n\ncustom_api.py\n file.\n\n\nExpected usage of \n{domain}\n is to get similar data \nfor different countries or regions by changing a little part of custom URL:\n\n\n   ru/series/CPI/m/2017  # country-level inflation for Russia \nru:77/series/CPI/m/2017  # inflation for Moscow region                         \n   kz/series/CPI/m/2017  # country-level inflation for Kazakhstan\n\n\n\n\nOutput format\n\n\nBy default custom API returns CSV file. This file is:\n\n\n\n\nviewable in browswer (download does not start)\n\n\nreadable by R/pandas\n\n\n\n\nOptional  \n{finaliser}\n may alter output format.\n\n\nImplementation\n\n\nCustom API currently developped and tested at \n\nown repo\n\nand mounted at \nfrontend app\n.\n\n\nNext steps\n\n\n\n\nMount custom API at db app",
            "title": "Custom API"
        },
        {
            "location": "/custom_api/#overview",
            "text": "Custom API is a simplified interface for end-user queries from database. \nIt uses long URL with slashes and no other parameters.  \u0421ustom API is intended to allow:   intuitive construction of URL for user  shorter notation than standard database API GET method   addressing several database API endpoints in one place  uniform call to same indicator for different countries or regions   API design originally discussed at  this issue .",
            "title": "Overview"
        },
        {
            "location": "/custom_api/#custom-api-translates-to-db-api",
            "text": "Custom API is essentially a thin syntax layer on top of database API. \nAll calls to custom API are redirected to database API.   For example, this call to custom API:   http://mini-kep.herokuapp.com/ru/series/CPI/m/rog/2015/2017  will return same data as:  https://minikep-db.herokuapp.com/api/datapoints?name=CPI_rog&freq=m&start_date=2015-01-01&end_date=2017-12-31",
            "title": "Custom API translates to db API"
        },
        {
            "location": "/custom_api/#url-syntax",
            "text": "Custom API url syntax is the following:  {domain}/series/{varname}/{freq}/{?suffix}/{?start}/{?end}/{?finaliser}\n\n? - optional\n\nExamples:\n   oil/series/BRENT/m/eop/2015/2017/csv\n   ru/series/EXPORT_GOODS/m/bln_rub     For further details, refer to the docstring in  custom_api.py  file.  Expected usage of  {domain}  is to get similar data \nfor different countries or regions by changing a little part of custom URL:     ru/series/CPI/m/2017  # country-level inflation for Russia \nru:77/series/CPI/m/2017  # inflation for Moscow region                         \n   kz/series/CPI/m/2017  # country-level inflation for Kazakhstan",
            "title": "URL syntax"
        },
        {
            "location": "/custom_api/#output-format",
            "text": "By default custom API returns CSV file. This file is:   viewable in browswer (download does not start)  readable by R/pandas   Optional   {finaliser}  may alter output format.",
            "title": "Output format"
        },
        {
            "location": "/custom_api/#implementation",
            "text": "Custom API currently developped and tested at  own repo \nand mounted at  frontend app .",
            "title": "Implementation"
        },
        {
            "location": "/custom_api/#next-steps",
            "text": "Mount custom API at db app",
            "title": "Next steps"
        },
        {
            "location": "/frontend/",
            "text": "Overview\n\n\nFrontend is a stand-alone flask app that is used to:\n- relay some existing data in experimental mode \n- make a list of indicators available in \nmini-kep\n database\n- show individual indicators homepages with charts, latest values \n  and instructions for download.\n\n\nRepository\n\n\nfrontend-app\n: \n\n\n \n\n\n\nIssues\n\n\n\n\nhomepage for individual indicators",
            "title": "Frontend"
        },
        {
            "location": "/frontend/#overview",
            "text": "Frontend is a stand-alone flask app that is used to:\n- relay some existing data in experimental mode \n- make a list of indicators available in  mini-kep  database\n- show individual indicators homepages with charts, latest values \n  and instructions for download.",
            "title": "Overview"
        },
        {
            "location": "/frontend/#repository",
            "text": "frontend-app :",
            "title": "Repository"
        },
        {
            "location": "/frontend/#issues",
            "text": "homepage for individual indicators",
            "title": "Issues"
        },
        {
            "location": "/datamodel_and_namespace/",
            "text": "Datamodel and enduser API\n\n\nThe idea of mini-kep project is to allow simple retrieval of Russian macroeconomic data \ninto python pandas and R, using an intuitive notation about variable naming and \nmodifiation.\n\n\nWe want an end-user to make a call like this to get qurterly nominal GDP and \nreal GDP growht rate: \n\n\n\ngdp_nominal = pd.read_json('https://mini-kep.herokuapp.com/ru/series/GDP/q')\ngdp_real_growth_rate = pd.read_json('https://mini-kep.herokuapp.com/ru/series/GDP/q/rog')\n\n\n\n\n\nrog\n is rate of growth, real growth rate to previous period \n\n\nDaily oil prices and exchange rates can be retrieved as: \n\n\n\nbrent = pd.read_json('https://mini-kep.herokuapp.com/oil/series/BRENT/d')\n#exchange rate\ner = pd.read_json('https://mini-kep.herokuapp.com/ru/series/USDRUR/d')\n\n\n\n\nAs of now the API frontend just shows how flask decodes long URL into a dict/json. You can click and see:\n\n\n\n\nhttps://mini-kep.herokuapp.com/ru/series/BRENT/m\n\n\nhttps://mini-kep.herokuapp.com/ru/series/BRENT/m/eop/2015/2016/csv\n\n\nhttps://mini-kep.herokuapp.com/ru/series/GDP/q/rog\n\n\n\n\nThe URL/API is styled around:\n- \nhttps://mini-kep.herokuapp.com/<domain>/series/<VARNAME>/<frequency>\n - mandatory part\n- \n<mandatory part>/<modifier | aggregator>/<start_year>/<end_year>/<finaliser>\n - optional part\n\n\nThe functionaly described not implemented yet, but hopefully soon be. For this to happen:\n- at \nparser-template\n we are working on importing parsing results (kep+exchange rate+oil) as dictionaries with datapoints \n- at \ndb\n the datapoints are to be stored in a database \n\n- \nthe frontend app\n should be able to query the database and provide real data for \n  calls above.\n\n\nAn important link that I propose to explore is what our datamodel for \nthe macroeconomic data and how to make good API documentation and prototype. \nAs for API the tricky part is making a 'reddit' style slashed API which \nassumes default values for some cases + some token combinations are invalid.\n\n\nMore to follow:\n- how we can get the data extracted (getter.py) + what is done with other parsers\n- naming convention in 'parser-rosstat-kep'\n- questions about data consistency and checks performed\n- 'true dataset' (affests API design and verification)\n- API blueprint\n- translate API call to database call (this should be sample code with a lot of test)\n- API v.0.1 vs features for the future\n\n\nSuggested reading/code\n\n\n\n\nread_csv()\n: \n\n\ngetter.py\n\n\nparser-rosstat-kep readme\n\n\napp frontpage code\n\n\n\n\n\n\nThis is a data access function used across the project to read the output as dataframes from stable URLs\nIn end-user API the import function will be  \npd.read_json\n as it will allow to bypass time index transformation:\n\n\n\n\nannual_CPI_rog = pd.read_json('http://mini-kep.herokuapp.com/ru/series/CPI/a/rog')\n\n\n\n\n\n\ndataframe consistency\n:\n\n\n\n\n\n\nThis module checks consistency of the results in the database. In particular, the monthly frequency \ndataframes are aggregated to an annual frequency, and checked against the published annual results. \nMore functions checking for consistency in other frequencies can be added (eg monthly to quarterly, quarterly to annual)\nWe found some inconsistencies were reported data on annual level is different that the log-sum on monthly rates,\neven controlling for rounding error.\n\n\n\n\n\n\nAPI design issue\n\n\n\n\n\n\nFrom here we need to extract the text on data rules and API documentation \n\n\n\n\nGlossary\n\n\n\n\ndata model\n\n\ntrue dataset\n\n\nreported dataset\n\n\nenduser API",
            "title": "Data model and namespace"
        },
        {
            "location": "/datamodel_and_namespace/#datamodel-and-enduser-api",
            "text": "The idea of mini-kep project is to allow simple retrieval of Russian macroeconomic data \ninto python pandas and R, using an intuitive notation about variable naming and \nmodifiation.  We want an end-user to make a call like this to get qurterly nominal GDP and \nreal GDP growht rate:   \ngdp_nominal = pd.read_json('https://mini-kep.herokuapp.com/ru/series/GDP/q')\ngdp_real_growth_rate = pd.read_json('https://mini-kep.herokuapp.com/ru/series/GDP/q/rog')  rog  is rate of growth, real growth rate to previous period   Daily oil prices and exchange rates can be retrieved as:   \nbrent = pd.read_json('https://mini-kep.herokuapp.com/oil/series/BRENT/d')\n#exchange rate\ner = pd.read_json('https://mini-kep.herokuapp.com/ru/series/USDRUR/d')  As of now the API frontend just shows how flask decodes long URL into a dict/json. You can click and see:   https://mini-kep.herokuapp.com/ru/series/BRENT/m  https://mini-kep.herokuapp.com/ru/series/BRENT/m/eop/2015/2016/csv  https://mini-kep.herokuapp.com/ru/series/GDP/q/rog   The URL/API is styled around:\n-  https://mini-kep.herokuapp.com/<domain>/series/<VARNAME>/<frequency>  - mandatory part\n-  <mandatory part>/<modifier | aggregator>/<start_year>/<end_year>/<finaliser>  - optional part  The functionaly described not implemented yet, but hopefully soon be. For this to happen:\n- at  parser-template  we are working on importing parsing results (kep+exchange rate+oil) as dictionaries with datapoints \n- at  db  the datapoints are to be stored in a database  \n-  the frontend app  should be able to query the database and provide real data for \n  calls above.  An important link that I propose to explore is what our datamodel for \nthe macroeconomic data and how to make good API documentation and prototype. \nAs for API the tricky part is making a 'reddit' style slashed API which \nassumes default values for some cases + some token combinations are invalid.  More to follow:\n- how we can get the data extracted (getter.py) + what is done with other parsers\n- naming convention in 'parser-rosstat-kep'\n- questions about data consistency and checks performed\n- 'true dataset' (affests API design and verification)\n- API blueprint\n- translate API call to database call (this should be sample code with a lot of test)\n- API v.0.1 vs features for the future",
            "title": "Datamodel and enduser API"
        },
        {
            "location": "/datamodel_and_namespace/#suggested-readingcode",
            "text": "read_csv() :   getter.py  parser-rosstat-kep readme  app frontpage code    This is a data access function used across the project to read the output as dataframes from stable URLs\nIn end-user API the import function will be   pd.read_json  as it will allow to bypass time index transformation:   annual_CPI_rog = pd.read_json('http://mini-kep.herokuapp.com/ru/series/CPI/a/rog')   dataframe consistency :    This module checks consistency of the results in the database. In particular, the monthly frequency \ndataframes are aggregated to an annual frequency, and checked against the published annual results. \nMore functions checking for consistency in other frequencies can be added (eg monthly to quarterly, quarterly to annual)\nWe found some inconsistencies were reported data on annual level is different that the log-sum on monthly rates,\neven controlling for rounding error.    API design issue    From here we need to extract the text on data rules and API documentation",
            "title": "Suggested reading/code"
        },
        {
            "location": "/datamodel_and_namespace/#glossary",
            "text": "data model  true dataset  reported dataset  enduser API",
            "title": "Glossary"
        },
        {
            "location": "/django_app/",
            "text": "Django db app\n\n\nThere is a supplementary dfango app for the db layer. We wanted to try both flask and django. With django the idea was that all components could go into one project.\n\n\ndjango \nfull-app\n:",
            "title": "Django app"
        },
        {
            "location": "/django_app/#django-db-app",
            "text": "There is a supplementary dfango app for the db layer. We wanted to try both flask and django. With django the idea was that all components could go into one project.  django  full-app :",
            "title": "Django db app"
        }
    ]
}