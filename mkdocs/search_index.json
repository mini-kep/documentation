{
    "docs": [
        {
            "location": "/",
            "text": "Introduction\n\n\nmini-kep\n is a small ETL (extract, transform, load) framework for \nRussian and global macroeconomic time series data with public end-user API.\n\n\nIt is inspired by \nSt Louis FRED\n and \n\nData Science Cookiecutter\n\nand aims to provide open, timely, machine-readable data for reproducible \nanalysis in economics.\n\n\nUser case\n\n\nWe assume an end user has some experience with \nFRED\n \nor \nquandl\n. \n\n\nFor his work an end user wants:\n\n\n\n\na clean dataset with latest data from different sources\n\n\nbrowse what data is available\n\n\nread this data on a local machine (in \npandas\n or \nR\n)\n\n\nquickly draw some charts like one below: \n\n\n\n\n\n\nExample: read official daily ruble/usd exchange rate from start of 2017\n\n\nimport pandas as pd\n\ndef read_ts(source_url):\n    \"\"\"Read pandas time series from *source_url*.\"\"\"\n    return pd.read_csv(source_url, \n                      converters={0: pd.to_datetime}, \n                      index_col=0,\n                      squeeze=True)\n\ner = read_ts('http://mini-kep.herokuapp.com/ru/series/USDRUR_CB/d/2017')\nassert er['2017-09-28'] == 58.01022\n\n\n\n\n\nClick \nhere\n to see same data in browser.\n\n\nGithub\n\n\nThis documentation source is stored on \nGithub",
            "title": "Overview"
        },
        {
            "location": "/#introduction",
            "text": "mini-kep  is a small ETL (extract, transform, load) framework for \nRussian and global macroeconomic time series data with public end-user API.  It is inspired by  St Louis FRED  and  Data Science Cookiecutter \nand aims to provide open, timely, machine-readable data for reproducible \nanalysis in economics.",
            "title": "Introduction"
        },
        {
            "location": "/#user-case",
            "text": "We assume an end user has some experience with  FRED  \nor  quandl .   For his work an end user wants:   a clean dataset with latest data from different sources  browse what data is available  read this data on a local machine (in  pandas  or  R )  quickly draw some charts like one below:     Example: read official daily ruble/usd exchange rate from start of 2017  import pandas as pd\n\ndef read_ts(source_url):\n    \"\"\"Read pandas time series from *source_url*.\"\"\"\n    return pd.read_csv(source_url, \n                      converters={0: pd.to_datetime}, \n                      index_col=0,\n                      squeeze=True)\n\ner = read_ts('http://mini-kep.herokuapp.com/ru/series/USDRUR_CB/d/2017')\nassert er['2017-09-28'] == 58.01022  Click  here  to see same data in browser.",
            "title": "User case"
        },
        {
            "location": "/#github",
            "text": "This documentation source is stored on  Github",
            "title": "Github"
        },
        {
            "location": "/pipeline/",
            "text": "Data pipleline\n\n\nmini-kep\n collects data from static files and public APIs,\nsaves it in database and provides end-user interface to browse this data \nand read it with R/pandas for visualisation and modelling.  \n\n\n1 \nParsers\n\n\n\n\ndownload data from sources (on static files or other APIs) \n\n\nassign variable names from common namespace \n\n\nemit stream of dictionaries like:  \n\n\n\n\n{'name': USDRUR_CB, 'date': '2017-09-28', 'freq': 'd', 'value': 58.0102}\n\n\n2 Scheduler\n\n\n\n\nestablish expected database content based on current date \n\n\nquery parsers for missing data \n\n\nupload to database\n\n\n\n\n3 \nDatabase\n\n\n\n\nflask app with SQLAlchemy and Postgres backend \n\n\nhas REST API to upload and \nretreive\n data\n\n\nhas custom API to retrieve data, custom API maps simplified query syntax to REST API\n\n\n\n\n4 \nFrontend app\n\n\n\n\nproject \nnews and how-to\n\n\nrelaying data from database query API\n\n\nhtml content for viewing data with charts (not implemented)\n\n\nsocial links for charts (not implemented)\n\n\n\n\n5 \nEnd-user examples\n\n\n\n\ndata access examples for end-user API\n\n\ncharting macroeconimic data\n\n\ncollection of Jupiter notebooks to demostrate data use (visualisation/modelling)\n\n\n\n\nComments\n\n\n\n\nPipeline is illustrated \nhere\n\n\nCommon namespace discussed \nhere",
            "title": "Data pipeline"
        },
        {
            "location": "/pipeline/#data-pipleline",
            "text": "mini-kep  collects data from static files and public APIs,\nsaves it in database and provides end-user interface to browse this data \nand read it with R/pandas for visualisation and modelling.",
            "title": "Data pipleline"
        },
        {
            "location": "/pipeline/#1-parsers",
            "text": "download data from sources (on static files or other APIs)   assign variable names from common namespace   emit stream of dictionaries like:     {'name': USDRUR_CB, 'date': '2017-09-28', 'freq': 'd', 'value': 58.0102}",
            "title": "1 Parsers"
        },
        {
            "location": "/pipeline/#2-scheduler",
            "text": "establish expected database content based on current date   query parsers for missing data   upload to database",
            "title": "2 Scheduler"
        },
        {
            "location": "/pipeline/#3-database",
            "text": "flask app with SQLAlchemy and Postgres backend   has REST API to upload and  retreive  data  has custom API to retrieve data, custom API maps simplified query syntax to REST API",
            "title": "3 Database"
        },
        {
            "location": "/pipeline/#4-frontend-app",
            "text": "project  news and how-to  relaying data from database query API  html content for viewing data with charts (not implemented)  social links for charts (not implemented)",
            "title": "4 Frontend app"
        },
        {
            "location": "/pipeline/#5-end-user-examples",
            "text": "data access examples for end-user API  charting macroeconimic data  collection of Jupiter notebooks to demostrate data use (visualisation/modelling)   Comments   Pipeline is illustrated  here  Common namespace discussed  here",
            "title": "5 End-user examples"
        },
        {
            "location": "/parsers/",
            "text": "Data sources and parsers\n\n\nData sources are static files (Word, PDF, html) and some open APIs. \nParser is the python code performing requests and emitting datapoints as dictionaries \nlike \n{'name': USDRUR_CB, 'date': '2017-09-28', 'freq': 'd', 'value': 58.0102}\n\n\nparser-rosstat-kep\n supplies most time monthly series. It is supplemented by daily ruble exchange rate from Bank of Russia and oil prices from EIA and some others.\n\n\nAggratation of parser data is made at \nparsers/runner.py\n\n\nRepository\n\n\nParsers\n:",
            "title": "Parsers"
        },
        {
            "location": "/parsers/#data-sources-and-parsers",
            "text": "Data sources are static files (Word, PDF, html) and some open APIs. \nParser is the python code performing requests and emitting datapoints as dictionaries \nlike  {'name': USDRUR_CB, 'date': '2017-09-28', 'freq': 'd', 'value': 58.0102}  parser-rosstat-kep  supplies most time monthly series. It is supplemented by daily ruble exchange rate from Bank of Russia and oil prices from EIA and some others.  Aggratation of parser data is made at  parsers/runner.py",
            "title": "Data sources and parsers"
        },
        {
            "location": "/parsers/#repository",
            "text": "Parsers :",
            "title": "Repository"
        },
        {
            "location": "/scheduler/",
            "text": "Scheduler\n\n\nScheduler is not implemented yet, but it is a cron-like task list to invoke parsers and upload data to database. \n\n\nConsider:\n\n\n\n\nHeroku APS\n\n\ncelery\n\n\nAPScheduler\n\n\nsomething else?",
            "title": "Scheduler"
        },
        {
            "location": "/scheduler/#scheduler",
            "text": "Scheduler is not implemented yet, but it is a cron-like task list to invoke parsers and upload data to database.   Consider:   Heroku APS  celery  APScheduler  something else?",
            "title": "Scheduler"
        },
        {
            "location": "/database/",
            "text": "Overview\n\n\nThis document describes initial requirement for database layer with two essential methods: \nPOST api\\incoming\n and \n\nGET api/datapoints\n. \n\n\nMore methods will be added to extend database functionality, \nplease refer to \ndb repository README.md\n for newest information. \n\n\nExpected functionality\n\n\n\n\nparser delivers a list of dicts, each dict is a datapoint\n\n\ndatabase should have a POST method at \napi\\incoming\n and write incoming json to db\n\n\nPOST operation must have some authentication\n\n\nfor simplicity all data is upserted - newer data always overwrites older data\n\n\ndatabase has GET method with datapoint keys as parameters (variable name, frequency),\n\n\nGET returns same format of output as incoming, output ordered by date, optionally  \n\n  filtered by start date and end date \n\n\n\n\nDatabase schema\n\n\nTable \nDatpoint\n\n\nId \u2013 UID, autoincrement  \nName \u2013 type String \\*  eg GDP\nFreq \u2013 type String \\*  \nDate \u2013 type DateTime \\*  2016-12-31\nValue \u2013 Float  \n\n\\* - composite key\n\n\n\n\nSee example at \nhttps://github.com/mini-kep/db/blob/master/demo/sqlalchemy/datapoint.py\n\n\nData structures\n\n\nIncoming / outgoing JSON\n\n\n[\n    {\n        \"date\": \"2016-06-30\",\n        \"freq\": \"m\",\n        \"name\": \"CPI_rog\",\n        \"value\": 100.4\n    },\n    {\n        \"date\": \"2016-06-30\",\n        \"freq\": \"m\",\n        \"name\": \"EXPORT_GOODS_bln_usd\",\n        \"value\": 24.0\n    },\n\n    # ...\n\n]   \n\n\n\n\nParser result is obtained \nhere\n\nfrom   \nDataset.yield_dicts(start='2017-01-01')\n. See \nDataset.serialise()\n for json creation.\nSample json is \nhere\n\n\nOutgoing CSV \n\n\n,GDP_yoy\n2016-03-31,99.6\n2016-06-30,99.5\n2016-09-30,99.6\n2016-12-31,100.3\n2017-03-31,100.5\n2017-06-30,102.5\n\n\n\n\nDatabase methods\n\n\nPOST\n\n\nPOST api/incoming\n \n\n\nValidates incoming json and upsert values to database. All fields should be filled.\n\n\nFor \ninsert_many()\n operation see \n'sheep/flock'\n example\n\n\nReturns:\n- empty JSON on success\n- error 400 on error in incoming json (eg invalid date string or empty parameter or missing field)\n\n\nPOST methods should require API_TOKEN as URL parameter or header, validate it with environment variable (possibly Heroku config vars)\n\n\nGET\n\n\nGET api/datapoints?name=<name>&freq=<freq>\nGET api/datapoints?name=<name>&freq=<freq>&start_date=<start_date>&end_date=<end_date>\n\n\n\n\nParameters:\n\n\n\n\nname\n (required) \u2013 name value to search like \nname=BRENT\n\n\nfreq\n (required) \u2013 freq value to search like \nfreq=m\n\n\nstart_date\n (optional) \u2013 should return results with date greater than this parameter\n\n\nend_date\n (optional) \u2013 should return results with date less than this parameter\n\n\nformat\n (optional, possible values \njson\n, \ncsv\n, default \ncsv\n) \u2013 returns data in chosen format. CSV data can be read by pandas with \npd.read_csv(url_to_api_request)\n\n\n\n\nReturns:\n\n\n\n\nCSV or JSON (default CSV) in format similar to incoming json with data sorted by date\n\n\nempty CSV or JSON if there\u2019s no data with such query.\n\n\nreturns error 400 on error in parameters\n\n\n\n\nTests (to edit)\n\n\nUpload data from JSON to DB, run python unit tests with requests to different methods, validate them with uploaded data.\n\n\nUse combinations GET \u2013 POST \u2013 GET to validate data inserts and updates.\n\n\nExample1\n\n\nExample2\n\n\nShould we write some \ntests in curl/httpie\n? \n\n\nTech stack\n\n\nWeb-frameworks: Flask + SQLAlchemy, alternative - Django, may consider Falcon\n\n\nContainer: prototype deployed to Heroku, alternative - use AWS EBTalk\n\n\nDatabase: Postgres (default on Heroku*), alternative - AWS RDS\n\n\nRepositories\n\n\ndb\n: \n\n\n\nNote\n\n\nThere is also \ndjango project for database\n, but it is on hold now.",
            "title": "Database"
        },
        {
            "location": "/database/#overview",
            "text": "This document describes initial requirement for database layer with two essential methods:  POST api\\incoming  and  GET api/datapoints .   More methods will be added to extend database functionality, \nplease refer to  db repository README.md  for newest information.",
            "title": "Overview"
        },
        {
            "location": "/database/#expected-functionality",
            "text": "parser delivers a list of dicts, each dict is a datapoint  database should have a POST method at  api\\incoming  and write incoming json to db  POST operation must have some authentication  for simplicity all data is upserted - newer data always overwrites older data  database has GET method with datapoint keys as parameters (variable name, frequency),  GET returns same format of output as incoming, output ordered by date, optionally   \n  filtered by start date and end date",
            "title": "Expected functionality"
        },
        {
            "location": "/database/#database-schema",
            "text": "Table  Datpoint  Id \u2013 UID, autoincrement  \nName \u2013 type String \\*  eg GDP\nFreq \u2013 type String \\*  \nDate \u2013 type DateTime \\*  2016-12-31\nValue \u2013 Float  \n\n\\* - composite key  See example at  https://github.com/mini-kep/db/blob/master/demo/sqlalchemy/datapoint.py",
            "title": "Database schema"
        },
        {
            "location": "/database/#data-structures",
            "text": "Incoming / outgoing JSON  [\n    {\n        \"date\": \"2016-06-30\",\n        \"freq\": \"m\",\n        \"name\": \"CPI_rog\",\n        \"value\": 100.4\n    },\n    {\n        \"date\": \"2016-06-30\",\n        \"freq\": \"m\",\n        \"name\": \"EXPORT_GOODS_bln_usd\",\n        \"value\": 24.0\n    },\n\n    # ...\n\n]     Parser result is obtained  here \nfrom    Dataset.yield_dicts(start='2017-01-01') . See  Dataset.serialise()  for json creation.\nSample json is  here  Outgoing CSV   ,GDP_yoy\n2016-03-31,99.6\n2016-06-30,99.5\n2016-09-30,99.6\n2016-12-31,100.3\n2017-03-31,100.5\n2017-06-30,102.5",
            "title": "Data structures"
        },
        {
            "location": "/database/#database-methods",
            "text": "",
            "title": "Database methods"
        },
        {
            "location": "/database/#post",
            "text": "POST api/incoming    Validates incoming json and upsert values to database. All fields should be filled.  For  insert_many()  operation see  'sheep/flock'  example  Returns:\n- empty JSON on success\n- error 400 on error in incoming json (eg invalid date string or empty parameter or missing field)  POST methods should require API_TOKEN as URL parameter or header, validate it with environment variable (possibly Heroku config vars)",
            "title": "POST"
        },
        {
            "location": "/database/#get",
            "text": "GET api/datapoints?name=<name>&freq=<freq>\nGET api/datapoints?name=<name>&freq=<freq>&start_date=<start_date>&end_date=<end_date>  Parameters:   name  (required) \u2013 name value to search like  name=BRENT  freq  (required) \u2013 freq value to search like  freq=m  start_date  (optional) \u2013 should return results with date greater than this parameter  end_date  (optional) \u2013 should return results with date less than this parameter  format  (optional, possible values  json ,  csv , default  csv ) \u2013 returns data in chosen format. CSV data can be read by pandas with  pd.read_csv(url_to_api_request)   Returns:   CSV or JSON (default CSV) in format similar to incoming json with data sorted by date  empty CSV or JSON if there\u2019s no data with such query.  returns error 400 on error in parameters",
            "title": "GET"
        },
        {
            "location": "/database/#tests-to-edit",
            "text": "Upload data from JSON to DB, run python unit tests with requests to different methods, validate them with uploaded data.  Use combinations GET \u2013 POST \u2013 GET to validate data inserts and updates.  Example1  Example2  Should we write some  tests in curl/httpie ?",
            "title": "Tests (to edit)"
        },
        {
            "location": "/database/#tech-stack",
            "text": "Web-frameworks: Flask + SQLAlchemy, alternative - Django, may consider Falcon  Container: prototype deployed to Heroku, alternative - use AWS EBTalk  Database: Postgres (default on Heroku*), alternative - AWS RDS",
            "title": "Tech stack"
        },
        {
            "location": "/database/#repositories",
            "text": "db :",
            "title": "Repositories"
        },
        {
            "location": "/database/#note",
            "text": "There is also  django project for database , but it is on hold now.",
            "title": "Note"
        },
        {
            "location": "/custom_api/",
            "text": "Overview\n\n\nCustom API is a simplified interface for end-user queries from database. \nIt uses long URL with slashes and no other parameters.\n\n\n\u0421ustom API is intended for:\n\n\n\n\nintuitive construction of URL by user\n\n\nshorter notation than standard database API GET method \n\n\naddressing several database API endpoints in one place\n\n\nuniform call to same indicator for different countries or regions\n\n\n\n\nAPI design originally discussed at \nthis issue\n.\n\n\nCustom API translates to db API\n\n\nCustom API is essentially a thin syntax layer on top of database API. \nAll calls to custom API are redirected to database API. \n\n\nFor example, this call to custom API: \n\n\nhttp://mini-kep.herokuapp.com/ru/series/CPI/m/rog/2015/2017\n\n\nwill return same data as:\n\n\nhttps://minikep-db.herokuapp.com/api/datapoints?name=CPI_rog&freq=m&start_date=2015-01-01&end_date=2017-12-31\n\n\nURL syntax\n\n\nCustom API URL syntax is the following:\n\n\n{domain}/series/{varname}/{freq}/{?suffix}/{?start}/{?end}/{?finaliser}\n\n? - optional\n\nExamples:\n   oil/series/BRENT/m/eop/2015/2017/csv\n   ru/series/EXPORT_GOODS/m/bln_rub   \n\n\n\n\nFor further details, refer to the docstring in \n\ncustom_api.py\n file.\n\n\nExpected usage of \n{domain}\n is to get similar data \nfor different countries or regions by changing a little part of custom URL:\n\n\n   ru/series/CPI/m/2017  # country-level inflation for Russia \nru:77/series/CPI/m/2017  # inflation for Moscow region                         \n   kz/series/CPI/m/2017  # country-level inflation for Kazakhstan\n\n\n\n\nOutput format\n\n\nBy default custom API returns CSV file. This file is:\n\n\n\n\nviewable in browser (download does not start)\n\n\nreadable by R/pandas\n\n\n\n\nOptional  \n{finaliser}\n may alter output format.\n\n\nImplementation\n\n\nCustom API currently developped and tested at \n\nown repo\n\nand mounted at \nfrontend app\n.\n\n\nNext steps\n\n\n\n\nMount custom API at db app",
            "title": "Custom API"
        },
        {
            "location": "/custom_api/#overview",
            "text": "Custom API is a simplified interface for end-user queries from database. \nIt uses long URL with slashes and no other parameters.  \u0421ustom API is intended for:   intuitive construction of URL by user  shorter notation than standard database API GET method   addressing several database API endpoints in one place  uniform call to same indicator for different countries or regions   API design originally discussed at  this issue .",
            "title": "Overview"
        },
        {
            "location": "/custom_api/#custom-api-translates-to-db-api",
            "text": "Custom API is essentially a thin syntax layer on top of database API. \nAll calls to custom API are redirected to database API.   For example, this call to custom API:   http://mini-kep.herokuapp.com/ru/series/CPI/m/rog/2015/2017  will return same data as:  https://minikep-db.herokuapp.com/api/datapoints?name=CPI_rog&freq=m&start_date=2015-01-01&end_date=2017-12-31",
            "title": "Custom API translates to db API"
        },
        {
            "location": "/custom_api/#url-syntax",
            "text": "Custom API URL syntax is the following:  {domain}/series/{varname}/{freq}/{?suffix}/{?start}/{?end}/{?finaliser}\n\n? - optional\n\nExamples:\n   oil/series/BRENT/m/eop/2015/2017/csv\n   ru/series/EXPORT_GOODS/m/bln_rub     For further details, refer to the docstring in  custom_api.py  file.  Expected usage of  {domain}  is to get similar data \nfor different countries or regions by changing a little part of custom URL:     ru/series/CPI/m/2017  # country-level inflation for Russia \nru:77/series/CPI/m/2017  # inflation for Moscow region                         \n   kz/series/CPI/m/2017  # country-level inflation for Kazakhstan",
            "title": "URL syntax"
        },
        {
            "location": "/custom_api/#output-format",
            "text": "By default custom API returns CSV file. This file is:   viewable in browser (download does not start)  readable by R/pandas   Optional   {finaliser}  may alter output format.",
            "title": "Output format"
        },
        {
            "location": "/custom_api/#implementation",
            "text": "Custom API currently developped and tested at  own repo \nand mounted at  frontend app .",
            "title": "Implementation"
        },
        {
            "location": "/custom_api/#next-steps",
            "text": "Mount custom API at db app",
            "title": "Next steps"
        },
        {
            "location": "/frontend/",
            "text": "Overview\n\n\nFrontend is a stand-alone flask app used to:\n\n\n\n\nrelay some existing data in experimental mode \n\n\nmake a list of indicators available in \nmini-kep\n database\n\n\nshow individual indicators homepages with charts, latest values \n  and instructions for download.\n\n\n\n\nRepository\n\n\nfrontend-app\n: \n\n \n\n\n\nIssues\n\n\n\n\nhomepage for individual indicators",
            "title": "Frontend"
        },
        {
            "location": "/frontend/#overview",
            "text": "Frontend is a stand-alone flask app used to:   relay some existing data in experimental mode   make a list of indicators available in  mini-kep  database  show individual indicators homepages with charts, latest values \n  and instructions for download.",
            "title": "Overview"
        },
        {
            "location": "/frontend/#repository",
            "text": "frontend-app :",
            "title": "Repository"
        },
        {
            "location": "/frontend/#issues",
            "text": "homepage for individual indicators",
            "title": "Issues"
        },
        {
            "location": "/notebooks/",
            "text": "Notebooks catalog\n\n\nThere is a \nrepo for use cases\n, but it is practically empty yet.\n\n\nThe use cases may resemble \ndatachart.cc\n\nor \ndatalab\n\nor \nviz-demo\n or some other repos (eg cmf-comovements).",
            "title": "Notebooks"
        },
        {
            "location": "/notebooks/#notebooks-catalog",
            "text": "There is a  repo for use cases , but it is practically empty yet.  The use cases may resemble  datachart.cc \nor  datalab \nor  viz-demo  or some other repos (eg cmf-comovements).",
            "title": "Notebooks catalog"
        },
        {
            "location": "/datamodel_and_namespace/",
            "text": "Datamodel and enduser API\n\n\nThe idea of mini-kep project is to allow simple retrieval of Russian macroeconomic data \ninto python pandas and R, using an intuitive notation about variable naming and \nmodifiation.\n\n\nWe want an end-user to make a call like this to get qurterly nominal GDP and \nreal GDP growth rate: \n\n\n\ngdp_nominal = pd.read_json('https://mini-kep.herokuapp.com/ru/series/GDP/q')\ngdp_real_growth_rate = pd.read_json('https://mini-kep.herokuapp.com/ru/series/GDP/q/rog')\n\n\n\n\n\nrog\n is rate of growth, real growth rate to previous period \n\n\nDaily oil prices and exchange rates can be retrieved as: \n\n\n\nbrent = pd.read_json('https://mini-kep.herokuapp.com/oil/series/BRENT/d')\n#exchange rate\ner = pd.read_json('https://mini-kep.herokuapp.com/ru/series/USDRUR/d')\n\n\n\n\nAs of now the API frontend just shows how flask decodes long URL into a dict/json. You can click and see:\n\n\n\n\nhttps://mini-kep.herokuapp.com/ru/series/BRENT/m\n\n\nhttps://mini-kep.herokuapp.com/ru/series/BRENT/m/eop/2015/2016/csv\n\n\nhttps://mini-kep.herokuapp.com/ru/series/GDP/q/rog\n\n\n\n\nThe URL/API is styled around:\n- \nhttps://mini-kep.herokuapp.com/<domain>/series/<VARNAME>/<frequency>\n - mandatory part\n- \n<mandatory part>/<modifier | aggregator>/<start_year>/<end_year>/<finaliser>\n - optional part\n\n\nThe functionaly described not implemented yet, but hopefully soon be. For this to happen:\n- at \nparser-template\n we are working on importing parsing results (kep+exchange rate+oil) as dictionaries with datapoints \n- at \ndb\n the datapoints are to be stored in a database \n\n- \nthe frontend app\n should be able to query the database and provide real data for \n  calls above.\n\n\nAn important link that I propose to explore is what our datamodel for \nthe macroeconomic data and how to make good API documentation and prototype. \nAs for API the tricky part is making a 'reddit' style slashed API which \nassumes default values for some cases + some token combinations are invalid.\n\n\nMore to follow:\n- how we can get the data extracted (getter.py) + what is done with other parsers\n- naming convention in 'parser-rosstat-kep'\n- questions about data consistency and checks performed\n- 'true dataset' (affests API design and verification)\n- API blueprint\n- translate API call to database call (this should be sample code with a lot of test)\n- API v.0.1 vs features for the future\n\n\nSuggested reading/code\n\n\n\n\nread_csv()\n: \n\n\ngetter.py\n\n\nparser-rosstat-kep readme\n\n\napp frontpage code\n\n\n\n\n\n\nThis is a data access function used across the project to read the output as dataframes from stable URLs\nIn end-user API the import function will be  \npd.read_json\n as it will allow to bypass time index transformation:\n\n\n\n\nannual_CPI_rog = pd.read_json('http://mini-kep.herokuapp.com/ru/series/CPI/a/rog')\n\n\n\n\n\n\ndataframe consistency\n:\n\n\n\n\n\n\nThis module checks consistency of the results in the database. In particular, the monthly frequency \ndataframes are aggregated to an annual frequency, and checked against the published annual results. \nMore functions checking for consistency in other frequencies can be added (eg monthly to quarterly, quarterly to annual)\nWe found some inconsistencies were reported data on annual level is different that the log-sum on monthly rates,\neven controlling for rounding error.\n\n\n\n\n\n\nAPI design issue\n\n\n\n\n\n\nFrom here we need to extract the text on data rules and API documentation \n\n\n\n\nGlossary\n\n\n\n\ndata model\n\n\ntrue dataset\n\n\nreported dataset\n\n\nenduser API",
            "title": "Data model and namespace"
        },
        {
            "location": "/datamodel_and_namespace/#datamodel-and-enduser-api",
            "text": "The idea of mini-kep project is to allow simple retrieval of Russian macroeconomic data \ninto python pandas and R, using an intuitive notation about variable naming and \nmodifiation.  We want an end-user to make a call like this to get qurterly nominal GDP and \nreal GDP growth rate:   \ngdp_nominal = pd.read_json('https://mini-kep.herokuapp.com/ru/series/GDP/q')\ngdp_real_growth_rate = pd.read_json('https://mini-kep.herokuapp.com/ru/series/GDP/q/rog')  rog  is rate of growth, real growth rate to previous period   Daily oil prices and exchange rates can be retrieved as:   \nbrent = pd.read_json('https://mini-kep.herokuapp.com/oil/series/BRENT/d')\n#exchange rate\ner = pd.read_json('https://mini-kep.herokuapp.com/ru/series/USDRUR/d')  As of now the API frontend just shows how flask decodes long URL into a dict/json. You can click and see:   https://mini-kep.herokuapp.com/ru/series/BRENT/m  https://mini-kep.herokuapp.com/ru/series/BRENT/m/eop/2015/2016/csv  https://mini-kep.herokuapp.com/ru/series/GDP/q/rog   The URL/API is styled around:\n-  https://mini-kep.herokuapp.com/<domain>/series/<VARNAME>/<frequency>  - mandatory part\n-  <mandatory part>/<modifier | aggregator>/<start_year>/<end_year>/<finaliser>  - optional part  The functionaly described not implemented yet, but hopefully soon be. For this to happen:\n- at  parser-template  we are working on importing parsing results (kep+exchange rate+oil) as dictionaries with datapoints \n- at  db  the datapoints are to be stored in a database  \n-  the frontend app  should be able to query the database and provide real data for \n  calls above.  An important link that I propose to explore is what our datamodel for \nthe macroeconomic data and how to make good API documentation and prototype. \nAs for API the tricky part is making a 'reddit' style slashed API which \nassumes default values for some cases + some token combinations are invalid.  More to follow:\n- how we can get the data extracted (getter.py) + what is done with other parsers\n- naming convention in 'parser-rosstat-kep'\n- questions about data consistency and checks performed\n- 'true dataset' (affests API design and verification)\n- API blueprint\n- translate API call to database call (this should be sample code with a lot of test)\n- API v.0.1 vs features for the future",
            "title": "Datamodel and enduser API"
        },
        {
            "location": "/datamodel_and_namespace/#suggested-readingcode",
            "text": "read_csv() :   getter.py  parser-rosstat-kep readme  app frontpage code    This is a data access function used across the project to read the output as dataframes from stable URLs\nIn end-user API the import function will be   pd.read_json  as it will allow to bypass time index transformation:   annual_CPI_rog = pd.read_json('http://mini-kep.herokuapp.com/ru/series/CPI/a/rog')   dataframe consistency :    This module checks consistency of the results in the database. In particular, the monthly frequency \ndataframes are aggregated to an annual frequency, and checked against the published annual results. \nMore functions checking for consistency in other frequencies can be added (eg monthly to quarterly, quarterly to annual)\nWe found some inconsistencies were reported data on annual level is different that the log-sum on monthly rates,\neven controlling for rounding error.    API design issue    From here we need to extract the text on data rules and API documentation",
            "title": "Suggested reading/code"
        },
        {
            "location": "/datamodel_and_namespace/#glossary",
            "text": "data model  true dataset  reported dataset  enduser API",
            "title": "Glossary"
        }
    ]
}